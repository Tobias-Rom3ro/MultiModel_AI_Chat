üß† Reflexi√≥n t√©cnica: aprendizajes, l√≠mites y mejoras

**Aprendizaje**

- **Arquitectura multi-proveedor**: abstraer llamadas a OpenRouter, Groq y Google AI detr√°s de un servicio com√∫n simplifica cambiar de modelo en caliente y facilita el manejo de errores/timeout por proveedor. Separar config/, prompts/, services/ y ui/ hizo el c√≥digo m√°s mantenible.
- **Modelos por tarea**: traducci√≥n y resumen funcionan bien con modelos ligeros; tareas multimodales (VQA) exigen modelos con visi√≥n y aumentan latencia por el procesamiento de la imagen. La app lo evidencia al mostrar tiempos de inferencia.
- **Dise√±o de prompts**: los prompts ‚Äúm√°s pesados‚Äù (m√°s contexto, restricciones y ejemplos) generan respuestas m√°s consistentes y alineadas con el formato pedido; a cambio consumen m√°s tokens, suben la latencia y el costo. Para traducci√≥n suele bastar un prompt m√≠nimo; para resumen y VQA conviene un marco expl√≠cito (objetivo, tono, longitud, y reglas como ‚Äúresponder solo con lo visible‚Äù).
- **UI centrada en flujo**: dropdowns de Proveedor/Tarea/Modelo + historial ayudan a comparar resultados r√°pido y a depurar problemas.

**L√≠mites encontrados**

- **Heterogeneidad de APIs**: cada proveedor tiene matices (nombres de modelos, formatos de mensajes, manejo de im√°genes). Sin una capa de adaptaci√≥n, aparecen errores sutiles.
- **Latencia y variabilidad**: cambia por tama√±o del modelo, carga del proveedor y ancho de banda (especialmente en VQA por el upload/encode de im√°genes). Algunos modelos no soportan ciertas tareas o devuelven errores cr√≠pticos.

**Posibles mejoras**

- Proveedor adicional (Ollama) para correr modelos locales y cumplir ‚Äúpuntos extra‚Äù.
- M√©tricas r√°pidas en UI: longitud del output, tokens estimados, costo aproximado y throughput.
- Streaming y cancelaci√≥n para mejorar UX en respuestas largas.
- Plantillas de prompt parametrizables (temperatura, longitud objetivo, estilo).
